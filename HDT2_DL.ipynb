{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOiGzxHxpr/HbVcWu8HVlwp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hoja de Trabajo 2\n",
        "# Deep Learning\n",
        "\n",
        "Autores:\n",
        "\n",
        "- Nelson García 22434\n",
        "- Joaquín Puente 22296"
      ],
      "metadata": {
        "id": "N-4vTJJJ8dJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 1 - Experimentación Práctica"
      ],
      "metadata": {
        "id": "nB2O34_W8q4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Preparación del conjunto de datos."
      ],
      "metadata": {
        "id": "kuQV_BAv8uN4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uyNxeDIQ8bqV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproducibilidad\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A4e02q-_SFP",
        "outputId": "8d4874ef-d6ed-4b68-92d5-91b5c16bbb09"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ae72a74bcd0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoDyT6He_YbO",
        "outputId": "79d620c2-2495-418a-931d-e17b4a747aef"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Cargar dataset Iris\n",
        "iris = load_iris()\n",
        "X = iris.data.astype(np.float32)     # sepal length/width, petal length/width\n",
        "y = iris.target.astype(np.int64)     # 3 clases: setosa, versicolor, virginica\n",
        "class_names = iris.target_names"
      ],
      "metadata": {
        "id": "cnl2RJJT_Y6g"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Train/Validation split estratificado (80/20)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
        ")"
      ],
      "metadata": {
        "id": "pqnwhpQK_Zg2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Normalización (fit SOLO en train, aplicar en val)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
        "X_val   = scaler.transform(X_val).astype(np.float32)\n"
      ],
      "metadata": {
        "id": "tWL3dEfUAvGD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Convertir a tensores\n",
        "X_train_t = torch.from_numpy(X_train)\n",
        "y_train_t = torch.from_numpy(y_train)\n",
        "X_val_t   = torch.from_numpy(X_val)\n",
        "y_val_t   = torch.from_numpy(y_val)"
      ],
      "metadata": {
        "id": "0QR17h_3AwFF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Crear TensorDatasets y DataLoaders\n",
        "batch_size = 16\n",
        "train_ds = TensorDataset(X_train_t, y_train_t)\n",
        "val_ds   = TensorDataset(X_val_t, y_val_t)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "LglxDzA-AwV5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Info útil para el modelado posterior\n",
        "input_dim  = X_train_t.shape[1]\n",
        "num_classes = len(np.unique(y))\n",
        "\n",
        "print(f\"Train size: {len(train_ds)} | Val size: {len(val_ds)}\")\n",
        "print(f\"Input dim: {input_dim} | Num classes: {num_classes}\")\n",
        "print(\"Clases:\", class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIIS033nAws9",
        "outputId": "6f1aff4a-f5a7-49bc-8ede-efc031fdda85"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 120 | Val size: 30\n",
            "Input dim: 4 | Num classes: 3\n",
            "Clases: ['setosa' 'versicolor' 'virginica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2 - Arquitectura modelo"
      ],
      "metadata": {
        "id": "jo-ZOMWoBV6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "U6RuXCihAw_d"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_activation(name: str) -> nn.Module:\n",
        "    name = name.lower()\n",
        "    if name == \"relu\":\n",
        "        return nn.ReLU(inplace=True)\n",
        "    if name == \"leaky_relu\":\n",
        "        return nn.LeakyReLU(0.1, inplace=True)\n",
        "    if name == \"tanh\":\n",
        "        return nn.Tanh()\n",
        "    if name == \"gelu\":\n",
        "        return nn.GELU()\n",
        "    if name == \"elu\":\n",
        "        return nn.ELU(inplace=True)\n",
        "    return nn.ReLU(inplace=True)"
      ],
      "metadata": {
        "id": "T8CGBwC1Cxq7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP para clasificación multiclase.\n",
        "    - Capa de entrada: input_dim\n",
        "    - Capas ocultas: hidden_dims (tupla/lista)\n",
        "    - Activación: configurable (relu, tanh, leaky_relu, gelu, elu)\n",
        "    - Capa de salida: num_classes logits (sin softmax; usarás CrossEntropyLoss)\n",
        "    - Opcional: BatchNorm y Dropout\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dims=(16, 8),\n",
        "        num_classes: int = 3,\n",
        "        activation: str = \"relu\",\n",
        "        dropout: float = 0.0,\n",
        "        batchnorm: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev = input_dim\n",
        "        act_layer = get_activation(activation)\n",
        "\n",
        "        for h in hidden_dims:\n",
        "            layers.append(nn.Linear(prev, h))\n",
        "            if batchnorm:\n",
        "                layers.append(nn.BatchNorm1d(h))\n",
        "            # activación\n",
        "            layers.append(get_activation(activation))\n",
        "            # dropout opcional\n",
        "            if dropout and dropout > 0:\n",
        "                layers.append(nn.Dropout(p=dropout))\n",
        "            prev = h\n",
        "\n",
        "        # Capa de salida (logits)\n",
        "        layers.append(nn.Linear(prev, num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "        # Inicialización de pesos acorde a la activación\n",
        "        self._init_weights(activation)\n",
        "\n",
        "    def _init_weights(self, activation: str):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                if activation.lower() in [\"relu\", \"leaky_relu\", \"elu\"]:\n",
        "                    nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
        "                elif activation.lower() in [\"tanh\"]:\n",
        "                    nn.init.xavier_normal_(m.weight, gain=nn.init.calculate_gain(\"tanh\"))\n",
        "                else:  # gelu u otras\n",
        "                    nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "bkQ8ZZ9JCyW4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Elecciones baselines (puedes cambiarlas para experimentar) ---\n",
        "hidden_dims = (16, 8)       # Nº neuronas por capa oculta\n",
        "activation  = \"relu\"        # \"relu\" | \"leaky_relu\" | \"tanh\" | \"gelu\" | \"elu\"\n",
        "dropout     = 0.0           # 0.0 por ahora; lo usaremos en regularización luego\n",
        "batchnorm   = False         # False por ahora\n",
        "\n",
        "model = MLP(\n",
        "    input_dim=input_dim,\n",
        "    hidden_dims=hidden_dims,\n",
        "    num_classes=num_classes,\n",
        "    activation=activation,\n",
        "    dropout=dropout,\n",
        "    batchnorm=batchnorm\n",
        ").to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUfg6p4nCzki",
        "outputId": "bfc680a4-5c90-4739-d8cc-bce602d36eed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=8, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conteo de parámetros entrenables\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Parámetros entrenables: {num_params:,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLiM6BvMCzdV",
        "outputId": "fade2d7d-0cfc-4eda-ee3c-2dbe558b231a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parámetros entrenables: 243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprobación rápida de shapes con un batch\n",
        "xb, yb = next(iter(train_loader))\n",
        "xb = xb.to(device)\n",
        "with torch.no_grad():\n",
        "    logits = model(xb)\n",
        "print(\"Logits shape:\", logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RED4gl5UCy84",
        "outputId": "82cfe7ea-0d50-41fc-dbb2-134b3395d108"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([16, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3 - Funciones de Pérdida"
      ],
      "metadata": {
        "id": "cHNRv9xaE0cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "0dE4quWgCypU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtV48GNEFsxz",
        "outputId": "10165444-b18e-4b82-a4e7-98f0d9c4cae9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ae72a74bcd0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_and_transform(name: str):\n",
        "    \"\"\"\n",
        "    Devuelve (criterion, transform) donde:\n",
        "    - criterion: función de pérdida de PyTorch\n",
        "    - transform(logits, y): adapta (logits, target) a lo que espera la pérdida\n",
        "      y retorna (pred_for_loss, target_for_loss)\n",
        "    \"\"\"\n",
        "    name = name.lower()\n",
        "    if name in (\"cross_entropy\", \"ce\", \"cross-entropy\"):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        def transform(logits, y):\n",
        "            # CrossEntropyLoss espera (logits, target_idx)\n",
        "            return logits, y\n",
        "        return criterion, transform\n",
        "\n",
        "    if name in (\"mse\", \"mse_loss\"):\n",
        "        criterion = nn.MSELoss()\n",
        "        def transform(logits, y):\n",
        "            # Para MSE, comparamos probabilidades softmax vs. one-hot del target\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            y_onehot = torch.zeros_like(probs)\n",
        "            y_onehot.scatter_(1, y.unsqueeze(1), 1.0)\n",
        "            return probs, y_onehot\n",
        "        return criterion, transform\n",
        "\n",
        "    if name in (\"multi_margin\", \"hinge\", \"svm\"):\n",
        "        # Hinge multiclass (SVM-style) incluida en PyTorch\n",
        "        criterion = nn.MultiMarginLoss()  # margin=1.0 por defecto\n",
        "        def transform(logits, y):\n",
        "            # MultiMarginLoss espera (scores, target_idx)\n",
        "            return logits, y\n",
        "        return criterion, transform\n",
        "\n",
        "    raise ValueError(f\"Pérdida no soportada: {name}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6VfyL88ZFu4k"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento genérico para una pérdida dada\n",
        "def train_with_loss(\n",
        "    loss_name: str,\n",
        "    epochs: int = 100,\n",
        "    lr: float = 1e-2,\n",
        "    weight_decay: float = 0.0,\n",
        "):\n",
        "    # Reinstanciar el modelo para cada pérdida (misma arquitectura)\n",
        "    try:\n",
        "        model_kwargs = dict(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dims=hidden_dims,\n",
        "            num_classes=num_classes,\n",
        "            activation=activation,\n",
        "            dropout=dropout,\n",
        "            batchnorm=batchnorm\n",
        "        )\n",
        "    except NameError:\n",
        "        model_kwargs = dict(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dims=(16, 8),\n",
        "            num_classes=num_classes,\n",
        "            activation=\"relu\",\n",
        "            dropout=0.0,\n",
        "            batchnorm=False\n",
        "        )\n",
        "\n",
        "    torch.manual_seed(RANDOM_STATE)\n",
        "    np.random.seed(RANDOM_STATE)\n",
        "    model = MLP(**model_kwargs).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion, transform = get_loss_and_transform(loss_name)\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [], \"val_loss\": [],\n",
        "        \"train_acc\":  [], \"val_acc\":  []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ---- TRAIN ----\n",
        "        model.train()\n",
        "        total_loss, total_correct, total_n = 0.0, 0, 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "\n",
        "            pred_for_loss, target_for_loss = transform(logits, yb)\n",
        "            loss = criterion(pred_for_loss, target_for_loss)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            total_correct += (logits.argmax(dim=1) == yb).sum().item()\n",
        "            total_n += xb.size(0)\n",
        "\n",
        "        train_loss = total_loss / total_n\n",
        "        train_acc  = total_correct / total_n\n",
        "\n",
        "        # ---- EVAL ----\n",
        "        model.eval()\n",
        "        val_total_loss, val_correct, val_n = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                pred_for_loss, target_for_loss = transform(logits, yb)\n",
        "                vloss = criterion(pred_for_loss, target_for_loss)\n",
        "                val_total_loss += vloss.item() * xb.size(0)\n",
        "                val_correct += (logits.argmax(dim=1) == yb).sum().item()\n",
        "                val_n += xb.size(0)\n",
        "\n",
        "        val_loss = val_total_loss / val_n\n",
        "        val_acc  = val_correct / val_n\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 1:\n",
        "            print(f\"[{loss_name}] Epoch {epoch+1:03d} | \"\n",
        "                  f\"train_loss={train_loss:.4f}, val_loss={val_loss:.4f} | \"\n",
        "                  f\"train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "X1-reTkuFvNB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Ejecutar experimentos con distintas pérdidas -----\n",
        "loss_names = [\"cross_entropy\", \"mse\", \"multi_margin\"]\n",
        "epochs = 100\n",
        "lr = 1e-2\n",
        "weight_decay = 0.0\n",
        "\n",
        "histories = {}\n",
        "final_summary = []\n",
        "\n",
        "for name in loss_names:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Entrenando con pérdida: {name}\")\n",
        "    print(\"=\"*70)\n",
        "    model_trained, hist = train_with_loss(\n",
        "        loss_name=name,\n",
        "        epochs=epochs,\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    histories[name] = hist\n",
        "    final_summary.append({\n",
        "        \"loss\": name,\n",
        "        \"final_train_loss\": hist[\"train_loss\"][-1],\n",
        "        \"final_val_loss\":   hist[\"val_loss\"][-1],\n",
        "        \"final_train_acc\":  hist[\"train_acc\"][-1],\n",
        "        \"final_val_acc\":    hist[\"val_acc\"][-1],\n",
        "    })\n",
        "\n",
        "print(\"\\n--- Resumen final ---\")\n",
        "for row in final_summary:\n",
        "    print(f\"{row['loss']:>13} | \"\n",
        "          f\"train_loss={row['final_train_loss']:.4f}, val_loss={row['final_val_loss']:.4f} | \"\n",
        "          f\"train_acc={row['final_train_acc']:.3f}, val_acc={row['final_val_acc']:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q86_R9CLFvWo",
        "outputId": "6aa8e0cc-105f-4af6-d964-26216701dc86"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Entrenando con pérdida: cross_entropy\n",
            "======================================================================\n",
            "[cross_entropy] Epoch 002 | train_loss=0.7919, val_loss=0.6962 | train_acc=0.758, val_acc=0.800\n",
            "[cross_entropy] Epoch 010 | train_loss=0.3986, val_loss=0.4194 | train_acc=0.900, val_acc=0.900\n",
            "[cross_entropy] Epoch 020 | train_loss=0.1847, val_loss=0.2216 | train_acc=0.958, val_acc=0.933\n",
            "[cross_entropy] Epoch 030 | train_loss=0.1034, val_loss=0.1541 | train_acc=0.975, val_acc=0.933\n",
            "[cross_entropy] Epoch 040 | train_loss=0.0788, val_loss=0.1405 | train_acc=0.992, val_acc=0.933\n",
            "[cross_entropy] Epoch 050 | train_loss=0.0584, val_loss=0.1151 | train_acc=0.983, val_acc=0.967\n",
            "[cross_entropy] Epoch 060 | train_loss=0.0464, val_loss=0.1157 | train_acc=0.983, val_acc=0.967\n",
            "[cross_entropy] Epoch 070 | train_loss=0.0444, val_loss=0.1085 | train_acc=0.975, val_acc=0.967\n",
            "[cross_entropy] Epoch 080 | train_loss=0.0363, val_loss=0.1527 | train_acc=0.975, val_acc=0.967\n",
            "[cross_entropy] Epoch 090 | train_loss=0.0253, val_loss=0.1423 | train_acc=0.992, val_acc=0.967\n",
            "[cross_entropy] Epoch 100 | train_loss=0.0217, val_loss=0.1723 | train_acc=0.992, val_acc=0.933\n",
            "\n",
            "======================================================================\n",
            "Entrenando con pérdida: mse\n",
            "======================================================================\n",
            "[mse] Epoch 002 | train_loss=0.1537, val_loss=0.1326 | train_acc=0.767, val_acc=0.800\n",
            "[mse] Epoch 010 | train_loss=0.0733, val_loss=0.0835 | train_acc=0.908, val_acc=0.867\n",
            "[mse] Epoch 020 | train_loss=0.0288, val_loss=0.0356 | train_acc=0.958, val_acc=0.933\n",
            "[mse] Epoch 030 | train_loss=0.0165, val_loss=0.0302 | train_acc=0.975, val_acc=0.933\n",
            "[mse] Epoch 040 | train_loss=0.0140, val_loss=0.0294 | train_acc=0.983, val_acc=0.967\n",
            "[mse] Epoch 050 | train_loss=0.0089, val_loss=0.0260 | train_acc=0.983, val_acc=0.933\n",
            "[mse] Epoch 060 | train_loss=0.0075, val_loss=0.0284 | train_acc=0.983, val_acc=0.933\n",
            "[mse] Epoch 070 | train_loss=0.0086, val_loss=0.0271 | train_acc=0.983, val_acc=0.933\n",
            "[mse] Epoch 080 | train_loss=0.0072, val_loss=0.0280 | train_acc=0.992, val_acc=0.967\n",
            "[mse] Epoch 090 | train_loss=0.0054, val_loss=0.0231 | train_acc=0.983, val_acc=0.967\n",
            "[mse] Epoch 100 | train_loss=0.0050, val_loss=0.0394 | train_acc=0.992, val_acc=0.867\n",
            "\n",
            "======================================================================\n",
            "Entrenando con pérdida: multi_margin\n",
            "======================================================================\n",
            "[multi_margin] Epoch 002 | train_loss=0.3835, val_loss=0.3379 | train_acc=0.825, val_acc=0.800\n",
            "[multi_margin] Epoch 010 | train_loss=0.0423, val_loss=0.0493 | train_acc=0.958, val_acc=0.933\n",
            "[multi_margin] Epoch 020 | train_loss=0.0277, val_loss=0.0433 | train_acc=0.958, val_acc=0.933\n",
            "[multi_margin] Epoch 030 | train_loss=0.0154, val_loss=0.0544 | train_acc=0.975, val_acc=0.967\n",
            "[multi_margin] Epoch 040 | train_loss=0.0136, val_loss=0.0539 | train_acc=0.983, val_acc=0.933\n",
            "[multi_margin] Epoch 050 | train_loss=0.0110, val_loss=0.0455 | train_acc=0.983, val_acc=0.933\n",
            "[multi_margin] Epoch 060 | train_loss=0.0076, val_loss=0.0467 | train_acc=1.000, val_acc=0.967\n",
            "[multi_margin] Epoch 070 | train_loss=0.0092, val_loss=0.0476 | train_acc=0.992, val_acc=0.933\n",
            "[multi_margin] Epoch 080 | train_loss=0.0078, val_loss=0.0511 | train_acc=1.000, val_acc=0.900\n",
            "[multi_margin] Epoch 090 | train_loss=0.0031, val_loss=0.0441 | train_acc=1.000, val_acc=0.967\n",
            "[multi_margin] Epoch 100 | train_loss=0.0020, val_loss=0.0424 | train_acc=1.000, val_acc=0.933\n",
            "\n",
            "--- Resumen final ---\n",
            "cross_entropy | train_loss=0.0217, val_loss=0.1723 | train_acc=0.992, val_acc=0.933\n",
            "          mse | train_loss=0.0050, val_loss=0.0394 | train_acc=0.992, val_acc=0.867\n",
            " multi_margin | train_loss=0.0020, val_loss=0.0424 | train_acc=1.000, val_acc=0.933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aVaR2DklFvfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UGq-ppy_Fvn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EII-dz0BFvvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h8r0gM4VFv1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X6_W4tKMFv7U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}