{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOeHm3TI8wnlXLhSlYZNHLA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hoja de Trabajo 2\n",
        "# Deep Learning\n",
        "\n",
        "Autores:\n",
        "\n",
        "- Nelson García 22434\n",
        "- Joaquín Puente 22296"
      ],
      "metadata": {
        "id": "N-4vTJJJ8dJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 1 - Experimentación Práctica"
      ],
      "metadata": {
        "id": "nB2O34_W8q4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Preparación del conjunto de datos."
      ],
      "metadata": {
        "id": "kuQV_BAv8uN4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uyNxeDIQ8bqV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproducibilidad\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A4e02q-_SFP",
        "outputId": "e92975e5-552b-41bd-aec1-c39a2c7a098b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x799482977e90>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoDyT6He_YbO",
        "outputId": "98871b33-b625-440c-f420-84cc6e52b6fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Cargar dataset Iris\n",
        "iris = load_iris()\n",
        "X = iris.data.astype(np.float32)     # sepal length/width, petal length/width\n",
        "y = iris.target.astype(np.int64)     # 3 clases: setosa, versicolor, virginica\n",
        "class_names = iris.target_names"
      ],
      "metadata": {
        "id": "cnl2RJJT_Y6g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Train/Validation split estratificado (80/20)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
        ")"
      ],
      "metadata": {
        "id": "pqnwhpQK_Zg2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Normalización (fit SOLO en train, aplicar en val)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
        "X_val   = scaler.transform(X_val).astype(np.float32)\n"
      ],
      "metadata": {
        "id": "tWL3dEfUAvGD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Convertir a tensores\n",
        "X_train_t = torch.from_numpy(X_train)\n",
        "y_train_t = torch.from_numpy(y_train)\n",
        "X_val_t   = torch.from_numpy(X_val)\n",
        "y_val_t   = torch.from_numpy(y_val)"
      ],
      "metadata": {
        "id": "0QR17h_3AwFF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Crear TensorDatasets y DataLoaders\n",
        "batch_size = 16\n",
        "train_ds = TensorDataset(X_train_t, y_train_t)\n",
        "val_ds   = TensorDataset(X_val_t, y_val_t)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "LglxDzA-AwV5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Info útil para el modelado posterior\n",
        "input_dim  = X_train_t.shape[1]\n",
        "num_classes = len(np.unique(y))\n",
        "\n",
        "print(f\"Train size: {len(train_ds)} | Val size: {len(val_ds)}\")\n",
        "print(f\"Input dim: {input_dim} | Num classes: {num_classes}\")\n",
        "print(\"Clases:\", class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIIS033nAws9",
        "outputId": "3fa1c405-7910-483d-fb81-e32a84319391"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 120 | Val size: 30\n",
            "Input dim: 4 | Num classes: 3\n",
            "Clases: ['setosa' 'versicolor' 'virginica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2 - Arquitectura modelo"
      ],
      "metadata": {
        "id": "jo-ZOMWoBV6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_activation(name: str) -> nn.Module:\n",
        "    name = name.lower()\n",
        "    if name == \"relu\":\n",
        "        return nn.ReLU(inplace=True)\n",
        "    if name == \"leaky_relu\":\n",
        "        return nn.LeakyReLU(0.1, inplace=True)\n",
        "    if name == \"tanh\":\n",
        "        return nn.Tanh()\n",
        "    if name == \"gelu\":\n",
        "        return nn.GELU()\n",
        "    if name == \"elu\":\n",
        "        return nn.ELU(inplace=True)\n",
        "    return nn.ReLU(inplace=True)"
      ],
      "metadata": {
        "id": "T8CGBwC1Cxq7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP para clasificación multiclase.\n",
        "    - Capa de entrada: input_dim\n",
        "    - Capas ocultas: hidden_dims (tupla/lista)\n",
        "    - Activación: configurable (relu, tanh, leaky_relu, gelu, elu)\n",
        "    - Capa de salida: num_classes logits (sin softmax; usarás CrossEntropyLoss)\n",
        "    - Opcional: BatchNorm y Dropout\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dims=(16, 8),\n",
        "        num_classes: int = 3,\n",
        "        activation: str = \"relu\",\n",
        "        dropout: float = 0.0,\n",
        "        batchnorm: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev = input_dim\n",
        "        act_layer = get_activation(activation)\n",
        "\n",
        "        for h in hidden_dims:\n",
        "            layers.append(nn.Linear(prev, h))\n",
        "            if batchnorm:\n",
        "                layers.append(nn.BatchNorm1d(h))\n",
        "            # activación\n",
        "            layers.append(get_activation(activation))\n",
        "            # dropout opcional\n",
        "            if dropout and dropout > 0:\n",
        "                layers.append(nn.Dropout(p=dropout))\n",
        "            prev = h\n",
        "\n",
        "        # Capa de salida (logits)\n",
        "        layers.append(nn.Linear(prev, num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "        # Inicialización de pesos acorde a la activación\n",
        "        self._init_weights(activation)\n",
        "\n",
        "    def _init_weights(self, activation: str):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                if activation.lower() in [\"relu\", \"leaky_relu\", \"elu\"]:\n",
        "                    nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
        "                elif activation.lower() in [\"tanh\"]:\n",
        "                    nn.init.xavier_normal_(m.weight, gain=nn.init.calculate_gain(\"tanh\"))\n",
        "                else:  # gelu u otras\n",
        "                    nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "bkQ8ZZ9JCyW4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Elecciones baselines (puedes cambiarlas para experimentar) ---\n",
        "hidden_dims = (16, 8)       # Nº neuronas por capa oculta\n",
        "activation  = \"relu\"        # \"relu\" | \"leaky_relu\" | \"tanh\" | \"gelu\" | \"elu\"\n",
        "dropout     = 0.0           # 0.0 por ahora; lo usaremos en regularización luego\n",
        "batchnorm   = False         # False por ahora\n",
        "\n",
        "model = MLP(\n",
        "    input_dim=input_dim,\n",
        "    hidden_dims=hidden_dims,\n",
        "    num_classes=num_classes,\n",
        "    activation=activation,\n",
        "    dropout=dropout,\n",
        "    batchnorm=batchnorm\n",
        ").to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUfg6p4nCzki",
        "outputId": "0b97a7fe-e261-49da-ed0a-308590781900"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=8, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conteo de parámetros entrenables\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Parámetros entrenables: {num_params:,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLiM6BvMCzdV",
        "outputId": "931acc45-74df-4995-a1cb-20eaa41be496"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parámetros entrenables: 243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprobación rápida de shapes con un batch\n",
        "xb, yb = next(iter(train_loader))\n",
        "xb = xb.to(device)\n",
        "with torch.no_grad():\n",
        "    logits = model(xb)\n",
        "print(\"Logits shape:\", logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RED4gl5UCy84",
        "outputId": "af0662bb-ecdd-46ae-b04f-9cb534b8c775"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([16, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3 - Funciones de Pérdida"
      ],
      "metadata": {
        "id": "cHNRv9xaE0cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtV48GNEFsxz",
        "outputId": "17971dd7-004a-4b3e-a015-5f158c6ef16f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x799482977e90>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_and_transform(name: str):\n",
        "    \"\"\"\n",
        "    Devuelve (criterion, transform) donde:\n",
        "    - criterion: función de pérdida de PyTorch\n",
        "    - transform(logits, y): adapta (logits, target) a lo que espera la pérdida\n",
        "      y retorna (pred_for_loss, target_for_loss)\n",
        "    \"\"\"\n",
        "    name = name.lower()\n",
        "    if name in (\"cross_entropy\", \"ce\", \"cross-entropy\"):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        def transform(logits, y):\n",
        "            # CrossEntropyLoss espera (logits, target_idx)\n",
        "            return logits, y\n",
        "        return criterion, transform\n",
        "\n",
        "    if name in (\"mse\", \"mse_loss\"):\n",
        "        criterion = nn.MSELoss()\n",
        "        def transform(logits, y):\n",
        "            # Para MSE, comparamos probabilidades softmax vs. one-hot del target\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            y_onehot = torch.zeros_like(probs)\n",
        "            y_onehot.scatter_(1, y.unsqueeze(1), 1.0)\n",
        "            return probs, y_onehot\n",
        "        return criterion, transform\n",
        "\n",
        "    if name in (\"multi_margin\", \"hinge\", \"svm\"):\n",
        "        # Hinge multiclass (SVM-style) incluida en PyTorch\n",
        "        criterion = nn.MultiMarginLoss()  # margin=1.0 por defecto\n",
        "        def transform(logits, y):\n",
        "            # MultiMarginLoss espera (scores, target_idx)\n",
        "            return logits, y\n",
        "        return criterion, transform\n",
        "\n",
        "    raise ValueError(f\"Pérdida no soportada: {name}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6VfyL88ZFu4k"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento genérico para una pérdida dada\n",
        "def train_with_loss(\n",
        "    loss_name: str,\n",
        "    epochs: int = 100,\n",
        "    lr: float = 1e-2,\n",
        "    weight_decay: float = 0.0,\n",
        "):\n",
        "    # Reinstanciar el modelo para cada pérdida (misma arquitectura)\n",
        "    try:\n",
        "        model_kwargs = dict(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dims=hidden_dims,\n",
        "            num_classes=num_classes,\n",
        "            activation=activation,\n",
        "            dropout=dropout,\n",
        "            batchnorm=batchnorm\n",
        "        )\n",
        "    except NameError:\n",
        "        model_kwargs = dict(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dims=(16, 8),\n",
        "            num_classes=num_classes,\n",
        "            activation=\"relu\",\n",
        "            dropout=0.0,\n",
        "            batchnorm=False\n",
        "        )\n",
        "\n",
        "    torch.manual_seed(RANDOM_STATE)\n",
        "    np.random.seed(RANDOM_STATE)\n",
        "    model = MLP(**model_kwargs).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion, transform = get_loss_and_transform(loss_name)\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [], \"val_loss\": [],\n",
        "        \"train_acc\":  [], \"val_acc\":  []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ---- TRAIN ----\n",
        "        model.train()\n",
        "        total_loss, total_correct, total_n = 0.0, 0, 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "\n",
        "            pred_for_loss, target_for_loss = transform(logits, yb)\n",
        "            loss = criterion(pred_for_loss, target_for_loss)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            total_correct += (logits.argmax(dim=1) == yb).sum().item()\n",
        "            total_n += xb.size(0)\n",
        "\n",
        "        train_loss = total_loss / total_n\n",
        "        train_acc  = total_correct / total_n\n",
        "\n",
        "        # ---- EVAL ----\n",
        "        model.eval()\n",
        "        val_total_loss, val_correct, val_n = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                pred_for_loss, target_for_loss = transform(logits, yb)\n",
        "                vloss = criterion(pred_for_loss, target_for_loss)\n",
        "                val_total_loss += vloss.item() * xb.size(0)\n",
        "                val_correct += (logits.argmax(dim=1) == yb).sum().item()\n",
        "                val_n += xb.size(0)\n",
        "\n",
        "        val_loss = val_total_loss / val_n\n",
        "        val_acc  = val_correct / val_n\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 1:\n",
        "            print(f\"[{loss_name}] Epoch {epoch+1:03d} | \"\n",
        "                  f\"train_loss={train_loss:.4f}, val_loss={val_loss:.4f} | \"\n",
        "                  f\"train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "X1-reTkuFvNB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Ejecutar experimentos con distintas pérdidas -----\n",
        "loss_names = [\"cross_entropy\", \"mse\", \"multi_margin\"]\n",
        "epochs = 100\n",
        "lr = 1e-2\n",
        "weight_decay = 0.0\n",
        "\n",
        "histories = {}\n",
        "final_summary = []\n",
        "\n",
        "for name in loss_names:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Entrenando con pérdida: {name}\")\n",
        "    print(\"=\"*70)\n",
        "    model_trained, hist = train_with_loss(\n",
        "        loss_name=name,\n",
        "        epochs=epochs,\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    histories[name] = hist\n",
        "    final_summary.append({\n",
        "        \"loss\": name,\n",
        "        \"final_train_loss\": hist[\"train_loss\"][-1],\n",
        "        \"final_val_loss\":   hist[\"val_loss\"][-1],\n",
        "        \"final_train_acc\":  hist[\"train_acc\"][-1],\n",
        "        \"final_val_acc\":    hist[\"val_acc\"][-1],\n",
        "    })\n",
        "\n",
        "print(\"\\n--- Resumen final ---\")\n",
        "for row in final_summary:\n",
        "    print(f\"{row['loss']:>13} | \"\n",
        "          f\"train_loss={row['final_train_loss']:.4f}, val_loss={row['final_val_loss']:.4f} | \"\n",
        "          f\"train_acc={row['final_train_acc']:.3f}, val_acc={row['final_val_acc']:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q86_R9CLFvWo",
        "outputId": "2a600d9e-08ab-41b0-cb17-160bc7b7d896"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Entrenando con pérdida: cross_entropy\n",
            "======================================================================\n",
            "[cross_entropy] Epoch 002 | train_loss=0.7919, val_loss=0.6962 | train_acc=0.758, val_acc=0.800\n",
            "[cross_entropy] Epoch 010 | train_loss=0.3986, val_loss=0.4194 | train_acc=0.900, val_acc=0.900\n",
            "[cross_entropy] Epoch 020 | train_loss=0.1847, val_loss=0.2216 | train_acc=0.958, val_acc=0.933\n",
            "[cross_entropy] Epoch 030 | train_loss=0.1034, val_loss=0.1541 | train_acc=0.975, val_acc=0.933\n",
            "[cross_entropy] Epoch 040 | train_loss=0.0788, val_loss=0.1405 | train_acc=0.992, val_acc=0.933\n",
            "[cross_entropy] Epoch 050 | train_loss=0.0584, val_loss=0.1151 | train_acc=0.983, val_acc=0.967\n",
            "[cross_entropy] Epoch 060 | train_loss=0.0464, val_loss=0.1157 | train_acc=0.983, val_acc=0.967\n",
            "[cross_entropy] Epoch 070 | train_loss=0.0444, val_loss=0.1085 | train_acc=0.975, val_acc=0.967\n",
            "[cross_entropy] Epoch 080 | train_loss=0.0363, val_loss=0.1527 | train_acc=0.975, val_acc=0.967\n",
            "[cross_entropy] Epoch 090 | train_loss=0.0253, val_loss=0.1423 | train_acc=0.992, val_acc=0.967\n",
            "[cross_entropy] Epoch 100 | train_loss=0.0217, val_loss=0.1723 | train_acc=0.992, val_acc=0.933\n",
            "\n",
            "======================================================================\n",
            "Entrenando con pérdida: mse\n",
            "======================================================================\n",
            "[mse] Epoch 002 | train_loss=0.1537, val_loss=0.1326 | train_acc=0.767, val_acc=0.800\n",
            "[mse] Epoch 010 | train_loss=0.0733, val_loss=0.0835 | train_acc=0.908, val_acc=0.867\n",
            "[mse] Epoch 020 | train_loss=0.0288, val_loss=0.0356 | train_acc=0.958, val_acc=0.933\n",
            "[mse] Epoch 030 | train_loss=0.0165, val_loss=0.0302 | train_acc=0.975, val_acc=0.933\n",
            "[mse] Epoch 040 | train_loss=0.0140, val_loss=0.0294 | train_acc=0.983, val_acc=0.967\n",
            "[mse] Epoch 050 | train_loss=0.0089, val_loss=0.0260 | train_acc=0.983, val_acc=0.933\n",
            "[mse] Epoch 060 | train_loss=0.0075, val_loss=0.0284 | train_acc=0.983, val_acc=0.933\n",
            "[mse] Epoch 070 | train_loss=0.0086, val_loss=0.0271 | train_acc=0.983, val_acc=0.933\n",
            "[mse] Epoch 080 | train_loss=0.0072, val_loss=0.0280 | train_acc=0.992, val_acc=0.967\n",
            "[mse] Epoch 090 | train_loss=0.0054, val_loss=0.0231 | train_acc=0.983, val_acc=0.967\n",
            "[mse] Epoch 100 | train_loss=0.0050, val_loss=0.0394 | train_acc=0.992, val_acc=0.867\n",
            "\n",
            "======================================================================\n",
            "Entrenando con pérdida: multi_margin\n",
            "======================================================================\n",
            "[multi_margin] Epoch 002 | train_loss=0.3835, val_loss=0.3379 | train_acc=0.825, val_acc=0.800\n",
            "[multi_margin] Epoch 010 | train_loss=0.0423, val_loss=0.0493 | train_acc=0.958, val_acc=0.933\n",
            "[multi_margin] Epoch 020 | train_loss=0.0277, val_loss=0.0433 | train_acc=0.958, val_acc=0.933\n",
            "[multi_margin] Epoch 030 | train_loss=0.0154, val_loss=0.0544 | train_acc=0.975, val_acc=0.967\n",
            "[multi_margin] Epoch 040 | train_loss=0.0136, val_loss=0.0539 | train_acc=0.983, val_acc=0.933\n",
            "[multi_margin] Epoch 050 | train_loss=0.0110, val_loss=0.0455 | train_acc=0.983, val_acc=0.933\n",
            "[multi_margin] Epoch 060 | train_loss=0.0076, val_loss=0.0467 | train_acc=1.000, val_acc=0.967\n",
            "[multi_margin] Epoch 070 | train_loss=0.0092, val_loss=0.0476 | train_acc=0.992, val_acc=0.933\n",
            "[multi_margin] Epoch 080 | train_loss=0.0078, val_loss=0.0511 | train_acc=1.000, val_acc=0.900\n",
            "[multi_margin] Epoch 090 | train_loss=0.0031, val_loss=0.0441 | train_acc=1.000, val_acc=0.967\n",
            "[multi_margin] Epoch 100 | train_loss=0.0020, val_loss=0.0424 | train_acc=1.000, val_acc=0.933\n",
            "\n",
            "--- Resumen final ---\n",
            "cross_entropy | train_loss=0.0217, val_loss=0.1723 | train_acc=0.992, val_acc=0.933\n",
            "          mse | train_loss=0.0050, val_loss=0.0394 | train_acc=0.992, val_acc=0.867\n",
            " multi_margin | train_loss=0.0020, val_loss=0.0424 | train_acc=1.000, val_acc=0.933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4 - Técnicas de Regularización"
      ],
      "metadata": {
        "id": "xyCiIjysryxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Utilidades ----------\n",
        "def l1_penalty(model: nn.Module, exclude_bias: bool = True):\n",
        "    \"\"\"Suma de |w| sobre los parámetros (opcionalmente excluye bias y batchnorm scales).\"\"\"\n",
        "    l1 = 0.0\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        if exclude_bias and (\".bias\" in name or \"bn\" in name.lower()):\n",
        "            continue\n",
        "        l1 = l1 + p.abs().sum()\n",
        "    return l1\n",
        "\n",
        "def make_model(dropout: float):\n",
        "    \"\"\"Instancia el MLP usando los hiperparámetros definidos en Task 2.\"\"\"\n",
        "    try:\n",
        "        model_kwargs = dict(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dims=hidden_dims,\n",
        "            num_classes=num_classes,\n",
        "            activation=activation,\n",
        "            dropout=dropout,\n",
        "            batchnorm=batchnorm\n",
        "        )\n",
        "    except NameError:\n",
        "        # Fallback si no existen variables de Task 2\n",
        "        model_kwargs = dict(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dims=(16, 8),\n",
        "            num_classes=num_classes,\n",
        "            activation=\"relu\",\n",
        "            dropout=dropout,\n",
        "            batchnorm=False\n",
        "        )\n",
        "    torch.manual_seed(RANDOM_STATE)\n",
        "    np.random.seed(RANDOM_STATE)\n",
        "    return MLP(**model_kwargs).to(device)\n",
        "\n",
        "def train_with_regularization(\n",
        "    loss_name: str = \"cross_entropy\",\n",
        "    epochs: int = 120,\n",
        "    lr: float = 1e-2,\n",
        "    dropout_p: float = 0.0,\n",
        "    l2_weight_decay: float = 0.0,\n",
        "    l1_lambda: float = 0.0,\n",
        "    early_stopping: bool = False,\n",
        "    patience: int = 20,\n",
        "):\n",
        "    \"\"\"\n",
        "    Entrena el modelo con las técnicas de regularización especificadas.\n",
        "    - L2: via weight_decay del optimizador\n",
        "    - L1: sum(|w|) añadido a la pérdida con coeficiente l1_lambda\n",
        "    - Dropout: probabilidad 'dropout_p' en el MLP\n",
        "    - Early Stopping (opcional): sobre val_loss con 'patience'\n",
        "    \"\"\"\n",
        "    model = make_model(dropout=dropout_p)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_weight_decay)\n",
        "    criterion, transform = get_loss_and_transform(loss_name)\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    best_state = None\n",
        "    bad_epochs = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ----- TRAIN -----\n",
        "        model.train()\n",
        "        total_loss, total_correct, total_n = 0.0, 0, 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(xb)\n",
        "            pred_for_loss, target_for_loss = transform(logits, yb)\n",
        "            loss = criterion(pred_for_loss, target_for_loss)\n",
        "\n",
        "            # L1 (si corresponde)\n",
        "            if l1_lambda and l1_lambda > 0:\n",
        "                loss = loss + l1_lambda * l1_penalty(model, exclude_bias=True)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            total_correct += (logits.argmax(dim=1) == yb).sum().item()\n",
        "            total_n += xb.size(0)\n",
        "\n",
        "        train_loss = total_loss / total_n\n",
        "        train_acc  = total_correct / total_n\n",
        "\n",
        "        # ----- EVAL -----\n",
        "        model.eval()\n",
        "        val_total_loss, val_correct, val_n = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                pred_for_loss, target_for_loss = transform(logits, yb)\n",
        "                vloss = criterion(pred_for_loss, target_for_loss)\n",
        "                # Nota: NO añadimos L1 en validación (se evalúa el loss \"puro\" del criterio)\n",
        "                val_total_loss += vloss.item() * xb.size(0)\n",
        "                val_correct += (logits.argmax(dim=1) == yb).sum().item()\n",
        "                val_n += xb.size(0)\n",
        "\n",
        "        val_loss = val_total_loss / val_n\n",
        "        val_acc  = val_correct / val_n\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        # Early stopping opcional\n",
        "        if early_stopping:\n",
        "            if val_loss < best_val - 1e-6:\n",
        "                best_val = val_loss\n",
        "                best_state = deepcopy(model.state_dict())\n",
        "                bad_epochs = 0\n",
        "            else:\n",
        "                bad_epochs += 1\n",
        "                if bad_epochs >= patience:\n",
        "                    print(f\"(Early Stopping) Epoch {epoch+1}: sin mejora por {patience} épocas.\")\n",
        "                    break\n",
        "\n",
        "        # Logging\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"[dropout={dropout_p:.2f} | L2={l2_weight_decay:.1e} | L1={l1_lambda:.1e}] \"\n",
        "                  f\"Epoch {epoch+1:03d} | \"\n",
        "                  f\"train_loss={train_loss:.4f}, val_loss={val_loss:.4f} | \"\n",
        "                  f\"train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n",
        "\n",
        "    # Cargar mejor estado si usamos early stopping\n",
        "    if early_stopping and best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "aVaR2DklFvfT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Experimentos ----------\n",
        "base_loss = \"cross_entropy\"\n",
        "\n",
        "configs = {\n",
        "    \"baseline_sin_reg\":   dict(dropout_p=0.0, l2_weight_decay=0.0,  l1_lambda=0.0,  early_stopping=False),\n",
        "    \"solo_L2_1e-3\":       dict(dropout_p=0.0, l2_weight_decay=1e-3, l1_lambda=0.0,  early_stopping=False),\n",
        "    \"solo_L1_1e-4\":       dict(dropout_p=0.0, l2_weight_decay=0.0,  l1_lambda=1e-4, early_stopping=False),\n",
        "    \"solo_Dropout_0.30\":  dict(dropout_p=0.30,l2_weight_decay=0.0,  l1_lambda=0.0,  early_stopping=False),\n",
        "    \"combo_L1+L2+DO\":     dict(dropout_p=0.25,l2_weight_decay=1e-3, l1_lambda=1e-4, early_stopping=True),\n",
        "}\n",
        "\n",
        "epochs = 140\n",
        "lr = 1e-2\n",
        "\n",
        "histories_reg = {}\n",
        "summary_reg = []\n",
        "\n",
        "for name, cfg in configs.items():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"Entrenando configuración: {name}\")\n",
        "    print(\"=\"*80)\n",
        "    model_reg, hist = train_with_regularization(\n",
        "        loss_name=base_loss,\n",
        "        epochs=epochs,\n",
        "        lr=lr,\n",
        "        **cfg\n",
        "    )\n",
        "    histories_reg[name] = hist\n",
        "    summary_reg.append({\n",
        "        \"config\": name,\n",
        "        \"final_train_loss\": hist[\"train_loss\"][-1],\n",
        "        \"final_val_loss\":   hist[\"val_loss\"][-1],\n",
        "        \"final_train_acc\":  hist[\"train_acc\"][-1],\n",
        "        \"final_val_acc\":    hist[\"val_acc\"][-1],\n",
        "        \"gap_acc\":          hist[\"train_acc\"][-1] - hist[\"val_acc\"][-1],  # indicio de overfitting\n",
        "    })\n",
        "\n",
        "print(\"\\n--- Resumen Regularización ---\")\n",
        "for row in summary_reg:\n",
        "    print(f\"{row['config']:<18} | \"\n",
        "          f\"train_loss={row['final_train_loss']:.4f}, val_loss={row['final_val_loss']:.4f} | \"\n",
        "          f\"train_acc={row['final_train_acc']:.3f}, val_acc={row['final_val_acc']:.3f} | \"\n",
        "          f\"gap_acc(train-val)={row['gap_acc']:.3f}\")"
      ],
      "metadata": {
        "id": "UGq-ppy_Fvn-",
        "outputId": "619d36f9-b666-4e65-9358-c626382d5a60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Entrenando configuración: baseline_sin_reg\n",
            "================================================================================\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=0.0e+00] Epoch 001 | train_loss=1.0716, val_loss=0.8357 | train_acc=0.358, val_acc=0.700\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=0.0e+00] Epoch 010 | train_loss=0.3986, val_loss=0.4194 | train_acc=0.900, val_acc=0.900\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=0.0e+00] Epoch 020 | train_loss=0.1847, val_loss=0.2216 | train_acc=0.958, val_acc=0.933\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=0.0e+00] Epoch 030 | train_loss=0.1034, val_loss=0.1541 | train_acc=0.975, val_acc=0.933\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=0.0e+00] Epoch 040 | train_loss=0.0788, val_loss=0.1405 | train_acc=0.992, val_acc=0.933\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=0.0e+00] Epoch 050 | train_loss=0.0584, val_loss=0.1151 | train_acc=0.983, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=0.0e+00] Epoch 060 | train_loss=0.0464, val_loss=0.1157 | train_acc=0.983, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=0.0e+00] Epoch 070 | train_loss=0.0444, val_loss=0.1085 | train_acc=0.975, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=0.0e+00] Epoch 080 | train_loss=0.0363, val_loss=0.1527 | train_acc=0.975, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=0.0e+00] Epoch 090 | train_loss=0.0253, val_loss=0.1423 | train_acc=0.992, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=0.0e+00] Epoch 100 | train_loss=0.0217, val_loss=0.1723 | train_acc=0.992, val_acc=0.933\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=0.0e+00] Epoch 110 | train_loss=0.0189, val_loss=0.1497 | train_acc=1.000, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=0.0e+00] Epoch 120 | train_loss=0.0176, val_loss=0.1693 | train_acc=1.000, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=0.0e+00] Epoch 130 | train_loss=0.0118, val_loss=0.1152 | train_acc=1.000, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=0.0e+00] Epoch 140 | train_loss=0.0093, val_loss=0.1127 | train_acc=1.000, val_acc=0.933\n",
            "\n",
            "================================================================================\n",
            "Entrenando configuración: solo_L2_1e-3\n",
            "================================================================================\n",
            "[dropout=0.00 | L2=1.0e-03 | L1=0.0e+00] Epoch 001 | train_loss=1.0711, val_loss=0.8349 | train_acc=0.367, val_acc=0.700\n",
            "[dropout=0.00 | L2=1.0e-03 | L1=0.0e+00] Epoch 010 | train_loss=0.1953, val_loss=0.2017 | train_acc=0.950, val_acc=0.933\n",
            "[dropout=0.00 | L2=1.0e-03 | L1=0.0e+00] Epoch 020 | train_loss=0.0704, val_loss=0.1075 | train_acc=0.967, val_acc=0.933\n",
            "[dropout=0.00 | L2=1.0e-03 | L1=0.0e+00] Epoch 030 | train_loss=0.0505, val_loss=0.1024 | train_acc=0.983, val_acc=0.933\n",
            "[dropout=0.00 | L2=1.0e-03 | L1=0.0e+00] Epoch 040 | train_loss=0.0621, val_loss=0.1115 | train_acc=0.983, val_acc=0.967\n",
            "[dropout=0.00 | L2=1.0e-03 | L1=0.0e+00] Epoch 050 | train_loss=0.0373, val_loss=0.0886 | train_acc=0.983, val_acc=0.967\n",
            "[dropout=0.00 | L2=1.0e-03 | L1=0.0e+00] Epoch 060 | train_loss=0.0340, val_loss=0.0845 | train_acc=0.983, val_acc=0.967\n",
            "[dropout=0.00 | L2=1.0e-03 | L1=0.0e+00] Epoch 070 | train_loss=0.0453, val_loss=0.0854 | train_acc=0.975, val_acc=0.933\n",
            "[dropout=0.00 | L2=1.0e-03 | L1=0.0e+00] Epoch 080 | train_loss=0.0414, val_loss=0.0790 | train_acc=0.983, val_acc=0.967\n",
            "[dropout=0.00 | L2=1.0e-03 | L1=0.0e+00] Epoch 090 | train_loss=0.0279, val_loss=0.0867 | train_acc=0.983, val_acc=0.967\n",
            "[dropout=0.00 | L2=1.0e-03 | L1=0.0e+00] Epoch 100 | train_loss=0.0239, val_loss=0.1342 | train_acc=0.992, val_acc=0.900\n",
            "[dropout=0.00 | L2=1.0e-03 | L1=0.0e+00] Epoch 110 | train_loss=0.0253, val_loss=0.1069 | train_acc=0.983, val_acc=0.967\n",
            "[dropout=0.00 | L2=1.0e-03 | L1=0.0e+00] Epoch 120 | train_loss=0.0307, val_loss=0.0877 | train_acc=0.983, val_acc=0.967\n",
            "[dropout=0.00 | L2=1.0e-03 | L1=0.0e+00] Epoch 130 | train_loss=0.0235, val_loss=0.1227 | train_acc=0.992, val_acc=0.967\n",
            "[dropout=0.00 | L2=1.0e-03 | L1=0.0e+00] Epoch 140 | train_loss=0.0224, val_loss=0.1028 | train_acc=0.983, val_acc=0.933\n",
            "\n",
            "================================================================================\n",
            "Entrenando configuración: solo_L1_1e-4\n",
            "================================================================================\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=1.0e-04] Epoch 001 | train_loss=1.0787, val_loss=0.8357 | train_acc=0.358, val_acc=0.700\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=1.0e-04] Epoch 010 | train_loss=0.2185, val_loss=0.2314 | train_acc=0.942, val_acc=0.900\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=1.0e-04] Epoch 020 | train_loss=0.0751, val_loss=0.1164 | train_acc=0.975, val_acc=0.933\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=1.0e-04] Epoch 030 | train_loss=0.0583, val_loss=0.1032 | train_acc=0.992, val_acc=0.933\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=1.0e-04] Epoch 040 | train_loss=0.0729, val_loss=0.1110 | train_acc=0.975, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=1.0e-04] Epoch 050 | train_loss=0.0433, val_loss=0.0962 | train_acc=0.983, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=1.0e-04] Epoch 060 | train_loss=0.0378, val_loss=0.0958 | train_acc=0.983, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=1.0e-04] Epoch 070 | train_loss=0.0486, val_loss=0.0699 | train_acc=0.975, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=1.0e-04] Epoch 080 | train_loss=0.0357, val_loss=0.0989 | train_acc=0.992, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=1.0e-04] Epoch 090 | train_loss=0.0264, val_loss=0.1290 | train_acc=0.992, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=1.0e-04] Epoch 100 | train_loss=0.0238, val_loss=0.1531 | train_acc=0.992, val_acc=0.900\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=1.0e-04] Epoch 110 | train_loss=0.0218, val_loss=0.1535 | train_acc=1.000, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=1.0e-04] Epoch 120 | train_loss=0.0260, val_loss=0.1169 | train_acc=0.992, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=1.0e-04] Epoch 130 | train_loss=0.0170, val_loss=0.1486 | train_acc=1.000, val_acc=0.967\n",
            "[dropout=0.00 | L2=0.0e+00 | L1=1.0e-04] Epoch 140 | train_loss=0.0153, val_loss=0.1493 | train_acc=1.000, val_acc=0.933\n",
            "\n",
            "================================================================================\n",
            "Entrenando configuración: solo_Dropout_0.30\n",
            "================================================================================\n",
            "[dropout=0.30 | L2=0.0e+00 | L1=0.0e+00] Epoch 001 | train_loss=1.2657, val_loss=0.8848 | train_acc=0.250, val_acc=0.700\n",
            "[dropout=0.30 | L2=0.0e+00 | L1=0.0e+00] Epoch 010 | train_loss=0.6157, val_loss=0.5056 | train_acc=0.758, val_acc=0.800\n",
            "[dropout=0.30 | L2=0.0e+00 | L1=0.0e+00] Epoch 020 | train_loss=0.3292, val_loss=0.2746 | train_acc=0.850, val_acc=0.800\n",
            "[dropout=0.30 | L2=0.0e+00 | L1=0.0e+00] Epoch 030 | train_loss=0.1385, val_loss=0.1379 | train_acc=0.950, val_acc=0.967\n",
            "[dropout=0.30 | L2=0.0e+00 | L1=0.0e+00] Epoch 040 | train_loss=0.2020, val_loss=0.0924 | train_acc=0.900, val_acc=0.967\n",
            "[dropout=0.30 | L2=0.0e+00 | L1=0.0e+00] Epoch 050 | train_loss=0.1167, val_loss=0.0956 | train_acc=0.950, val_acc=0.933\n",
            "[dropout=0.30 | L2=0.0e+00 | L1=0.0e+00] Epoch 060 | train_loss=0.1629, val_loss=0.0913 | train_acc=0.950, val_acc=0.967\n",
            "[dropout=0.30 | L2=0.0e+00 | L1=0.0e+00] Epoch 070 | train_loss=0.1189, val_loss=0.1041 | train_acc=0.958, val_acc=0.967\n",
            "[dropout=0.30 | L2=0.0e+00 | L1=0.0e+00] Epoch 080 | train_loss=0.1317, val_loss=0.0910 | train_acc=0.933, val_acc=0.967\n",
            "[dropout=0.30 | L2=0.0e+00 | L1=0.0e+00] Epoch 090 | train_loss=0.0933, val_loss=0.0582 | train_acc=0.942, val_acc=1.000\n",
            "[dropout=0.30 | L2=0.0e+00 | L1=0.0e+00] Epoch 100 | train_loss=0.1059, val_loss=0.0895 | train_acc=0.967, val_acc=0.933\n",
            "[dropout=0.30 | L2=0.0e+00 | L1=0.0e+00] Epoch 110 | train_loss=0.0786, val_loss=0.0945 | train_acc=0.975, val_acc=0.933\n",
            "[dropout=0.30 | L2=0.0e+00 | L1=0.0e+00] Epoch 120 | train_loss=0.1120, val_loss=0.0485 | train_acc=0.967, val_acc=1.000\n",
            "[dropout=0.30 | L2=0.0e+00 | L1=0.0e+00] Epoch 130 | train_loss=0.0622, val_loss=0.0689 | train_acc=0.975, val_acc=0.967\n",
            "[dropout=0.30 | L2=0.0e+00 | L1=0.0e+00] Epoch 140 | train_loss=0.1856, val_loss=0.0902 | train_acc=0.933, val_acc=0.967\n",
            "\n",
            "================================================================================\n",
            "Entrenando configuración: combo_L1+L2+DO\n",
            "================================================================================\n",
            "[dropout=0.25 | L2=1.0e-03 | L1=1.0e-04] Epoch 001 | train_loss=1.2214, val_loss=0.8711 | train_acc=0.267, val_acc=0.767\n",
            "[dropout=0.25 | L2=1.0e-03 | L1=1.0e-04] Epoch 010 | train_loss=0.4604, val_loss=0.3694 | train_acc=0.817, val_acc=0.767\n",
            "[dropout=0.25 | L2=1.0e-03 | L1=1.0e-04] Epoch 020 | train_loss=0.2400, val_loss=0.1863 | train_acc=0.900, val_acc=0.933\n",
            "[dropout=0.25 | L2=1.0e-03 | L1=1.0e-04] Epoch 030 | train_loss=0.1048, val_loss=0.1179 | train_acc=0.975, val_acc=0.967\n",
            "[dropout=0.25 | L2=1.0e-03 | L1=1.0e-04] Epoch 040 | train_loss=0.1408, val_loss=0.0859 | train_acc=0.942, val_acc=0.967\n",
            "[dropout=0.25 | L2=1.0e-03 | L1=1.0e-04] Epoch 050 | train_loss=0.0934, val_loss=0.0809 | train_acc=0.958, val_acc=0.967\n",
            "[dropout=0.25 | L2=1.0e-03 | L1=1.0e-04] Epoch 060 | train_loss=0.0960, val_loss=0.0747 | train_acc=0.967, val_acc=0.967\n",
            "[dropout=0.25 | L2=1.0e-03 | L1=1.0e-04] Epoch 070 | train_loss=0.1122, val_loss=0.0839 | train_acc=0.967, val_acc=0.967\n",
            "[dropout=0.25 | L2=1.0e-03 | L1=1.0e-04] Epoch 080 | train_loss=0.1819, val_loss=0.0814 | train_acc=0.933, val_acc=0.967\n",
            "(Early Stopping) Epoch 81: sin mejora por 20 épocas.\n",
            "\n",
            "--- Resumen Regularización ---\n",
            "baseline_sin_reg   | train_loss=0.0093, val_loss=0.1127 | train_acc=1.000, val_acc=0.933 | gap_acc(train-val)=0.067\n",
            "solo_L2_1e-3       | train_loss=0.0224, val_loss=0.1028 | train_acc=0.983, val_acc=0.933 | gap_acc(train-val)=0.050\n",
            "solo_L1_1e-4       | train_loss=0.0153, val_loss=0.1493 | train_acc=1.000, val_acc=0.933 | gap_acc(train-val)=0.067\n",
            "solo_Dropout_0.30  | train_loss=0.1856, val_loss=0.0902 | train_acc=0.933, val_acc=0.967 | gap_acc(train-val)=-0.033\n",
            "combo_L1+L2+DO     | train_loss=0.0735, val_loss=0.0767 | train_acc=0.992, val_acc=1.000 | gap_acc(train-val)=-0.008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Algoritmos de Optimización"
      ],
      "metadata": {
        "id": "-FTGsBTouq9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "EII-dz0BFvvJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(dropout_p: float = 0.0):\n",
        "    \"\"\"Instancia el MLP con los hiperparámetros del Task 2.\"\"\"\n",
        "    try:\n",
        "        model_kwargs = dict(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dims=hidden_dims,\n",
        "            num_classes=num_classes,\n",
        "            activation=activation,\n",
        "            dropout=dropout_p if 'dropout' in globals() else 0.0,\n",
        "            batchnorm=batchnorm if 'batchnorm' in globals() else False\n",
        "        )\n",
        "    except NameError:\n",
        "        model_kwargs = dict(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dims=(16, 8),\n",
        "            num_classes=num_classes,\n",
        "            activation=\"relu\",\n",
        "            dropout=0.0,\n",
        "            batchnorm=False\n",
        "        )\n",
        "    torch.manual_seed(RANDOM_STATE)\n",
        "    np.random.seed(RANDOM_STATE)\n",
        "    return MLP(**model_kwargs).to(device)\n",
        "\n",
        "def make_loaders(mode: str, mini_bs: int = 16):\n",
        "    \"\"\"\n",
        "    Crea DataLoaders según el 'algoritmo':\n",
        "    - 'batch' -> Batch GD (batch = N)\n",
        "    - 'mini'  -> Mini-Batch GD (batch = mini_bs)\n",
        "    - 'sgd'   -> SGD online (batch = 1)\n",
        "    \"\"\"\n",
        "    N_train = len(train_ds)\n",
        "    if mode == \"batch\":\n",
        "        train_loader = DataLoader(train_ds, batch_size=N_train, shuffle=False)\n",
        "    elif mode == \"mini\":\n",
        "        train_loader = DataLoader(train_ds, batch_size=mini_bs, shuffle=True)\n",
        "    elif mode == \"sgd\":\n",
        "        train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
        "    else:\n",
        "        raise ValueError(f\"Modo no soportado: {mode}\")\n",
        "\n",
        "    # Para validación usamos batch \"grande\" para acelerar y medir tiempo de eval de forma consistente\n",
        "    val_loader = DataLoader(val_ds, batch_size=len(val_ds), shuffle=False)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def train_with_optimizer_mode(\n",
        "    mode: str,                       # 'batch' | 'mini' | 'sgd'\n",
        "    epochs: int = 120,\n",
        "    lr: float = 1e-2,\n",
        "    mini_bs: int = 16,\n",
        "    loss_name: str = \"cross_entropy\",\n",
        "    weight_decay: float = 0.0,\n",
        "    dropout_p: float = 0.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Entrena usando diferentes estrategias de cálculo del gradiente según el tamaño de batch.\n",
        "    Optimiza con torch.optim.SGD (sin momentum para comparar \"limpio\").\n",
        "    Registra pérdidas, accuracy y tiempos (train y val) por época.\n",
        "    \"\"\"\n",
        "    model = make_model(dropout_p)\n",
        "    train_loader, val_loader = make_loaders(mode, mini_bs=mini_bs)\n",
        "    criterion, transform = get_loss_and_transform(loss_name)\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [], \"val_loss\": [],\n",
        "        \"train_acc\":  [], \"val_acc\":  [],\n",
        "        \"train_time\": [], \"val_time\": []\n",
        "    }\n",
        "\n",
        "    updates_per_epoch = len(train_loader)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ----- TRAIN -----\n",
        "        t0 = time.perf_counter()\n",
        "        model.train()\n",
        "        total_loss, total_correct, total_n = 0.0, 0, 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            pred_for_loss, target_for_loss = transform(logits, yb)\n",
        "            loss = criterion(pred_for_loss, target_for_loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            total_correct += (logits.argmax(dim=1) == yb).sum().item()\n",
        "            total_n += xb.size(0)\n",
        "\n",
        "        train_time = time.perf_counter() - t0\n",
        "        train_loss = total_loss / total_n\n",
        "        train_acc  = total_correct / total_n\n",
        "\n",
        "        # ----- EVAL -----\n",
        "        t1 = time.perf_counter()\n",
        "        model.eval()\n",
        "        val_total_loss, val_correct, val_n = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                pred_for_loss, target_for_loss = transform(logits, yb)\n",
        "                vloss = criterion(pred_for_loss, target_for_loss)\n",
        "                val_total_loss += vloss.item() * xb.size(0)\n",
        "                val_correct += (logits.argmax(dim=1) == yb).sum().item()\n",
        "                val_n += xb.size(0)\n",
        "        val_time = time.perf_counter() - t1\n",
        "\n",
        "        val_loss = val_total_loss / val_n\n",
        "        val_acc  = val_correct / val_n\n",
        "\n",
        "        # Guardar métricas\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        history[\"train_time\"].append(train_time)\n",
        "        history[\"val_time\"].append(val_time)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"[{mode.upper():>5} | updates/epoch={updates_per_epoch}] \"\n",
        "                  f\"Epoch {epoch+1:03d} | \"\n",
        "                  f\"train_loss={train_loss:.4f}, val_loss={val_loss:.4f} | \"\n",
        "                  f\"train_acc={train_acc:.3f}, val_acc={val_acc:.3f} | \"\n",
        "                  f\"t_train={train_time:.3f}s, t_val={val_time:.3f}s\")\n",
        "\n",
        "    # Resumen de tiempos totales\n",
        "    history[\"total_train_time\"] = float(np.sum(history[\"train_time\"]))\n",
        "    history[\"total_val_time\"]   = float(np.sum(history[\"val_time\"]))\n",
        "    return model, history, updates_per_epoch"
      ],
      "metadata": {
        "id": "h8r0gM4VFv1c"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 120\n",
        "base_lr = 1e-2\n",
        "mini_bs = 16\n",
        "loss_name = \"cross_entropy\"\n",
        "\n",
        "experiments = [\n",
        "    (\"Batch GD\",   \"batch\", None),\n",
        "    (f\"Mini-Batch (bs={mini_bs})\", \"mini\", mini_bs),\n",
        "    (\"SGD (bs=1)\", \"sgd\", 1),\n",
        "]\n",
        "\n",
        "results = []\n",
        "histories_opt = {}\n",
        "\n",
        "for label, mode, mbs in experiments:\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(f\"Entrenando con: {label}\")\n",
        "    print(\"=\"*90)\n",
        "    _, hist, steps = train_with_optimizer_mode(\n",
        "        mode=mode,\n",
        "        epochs=epochs,\n",
        "        lr=base_lr,\n",
        "        mini_bs=mini_bs if mbs is None else mbs,\n",
        "        loss_name=loss_name,\n",
        "        weight_decay=0.001,\n",
        "        dropout_p=0.25,\n",
        "    )\n",
        "    histories_opt[label] = hist\n",
        "    results.append({\n",
        "        \"algoritmo\": label,\n",
        "        \"updates_por_epoca\": steps,\n",
        "        \"final_train_loss\": hist[\"train_loss\"][-1],\n",
        "        \"final_val_loss\":   hist[\"val_loss\"][-1],\n",
        "        \"final_train_acc\":  hist[\"train_acc\"][-1],\n",
        "        \"final_val_acc\":    hist[\"val_acc\"][-1],\n",
        "        \"tiempo_train_total_s\": hist[\"total_train_time\"],\n",
        "        \"tiempo_val_total_s\":   hist[\"total_val_time\"],\n",
        "    })\n",
        "\n",
        "print(\"\\n--- Resumen Optimización ---\")\n",
        "for r in results:\n",
        "    print(f\"{r['algoritmo']:<24} | upd/ep={r['updates_por_epoca']:<3} | \"\n",
        "          f\"train_loss={r['final_train_loss']:.4f}, val_loss={r['final_val_loss']:.4f} | \"\n",
        "          f\"train_acc={r['final_train_acc']:.3f}, val_acc={r['final_val_acc']:.3f} | \"\n",
        "          f\"T_train={r['tiempo_train_total_s']:.2f}s, T_val={r['tiempo_val_total_s']:.2f}s\")\n"
      ],
      "metadata": {
        "id": "X6_W4tKMFv7U",
        "outputId": "43bc1c41-4f4e-4eb2-f7d4-ef28fad9640f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========================================================================================\n",
            "Entrenando con: Batch GD\n",
            "==========================================================================================\n",
            "[BATCH | updates/epoch=1] Epoch 001 | train_loss=1.4984, val_loss=1.2163 | train_acc=0.200, val_acc=0.133 | t_train=0.004s, t_val=0.001s\n",
            "[BATCH | updates/epoch=1] Epoch 010 | train_loss=1.3493, val_loss=1.0653 | train_acc=0.292, val_acc=0.133 | t_train=0.002s, t_val=0.001s\n",
            "[BATCH | updates/epoch=1] Epoch 020 | train_loss=1.2253, val_loss=0.9771 | train_acc=0.267, val_acc=0.233 | t_train=0.002s, t_val=0.001s\n",
            "[BATCH | updates/epoch=1] Epoch 030 | train_loss=1.1302, val_loss=0.9224 | train_acc=0.417, val_acc=0.467 | t_train=0.002s, t_val=0.001s\n",
            "[BATCH | updates/epoch=1] Epoch 040 | train_loss=1.1087, val_loss=0.8825 | train_acc=0.408, val_acc=0.667 | t_train=0.002s, t_val=0.001s\n",
            "[BATCH | updates/epoch=1] Epoch 050 | train_loss=0.9864, val_loss=0.8513 | train_acc=0.492, val_acc=0.733 | t_train=0.002s, t_val=0.001s\n",
            "[BATCH | updates/epoch=1] Epoch 060 | train_loss=0.9821, val_loss=0.8255 | train_acc=0.425, val_acc=0.733 | t_train=0.002s, t_val=0.001s\n",
            "[BATCH | updates/epoch=1] Epoch 070 | train_loss=0.9511, val_loss=0.8045 | train_acc=0.475, val_acc=0.733 | t_train=0.002s, t_val=0.001s\n",
            "[BATCH | updates/epoch=1] Epoch 080 | train_loss=0.9638, val_loss=0.7864 | train_acc=0.575, val_acc=0.733 | t_train=0.002s, t_val=0.001s\n",
            "[BATCH | updates/epoch=1] Epoch 090 | train_loss=0.9019, val_loss=0.7697 | train_acc=0.517, val_acc=0.767 | t_train=0.002s, t_val=0.001s\n",
            "[BATCH | updates/epoch=1] Epoch 100 | train_loss=0.9470, val_loss=0.7550 | train_acc=0.525, val_acc=0.800 | t_train=0.002s, t_val=0.001s\n",
            "[BATCH | updates/epoch=1] Epoch 110 | train_loss=0.8064, val_loss=0.7398 | train_acc=0.608, val_acc=0.800 | t_train=0.002s, t_val=0.001s\n",
            "[BATCH | updates/epoch=1] Epoch 120 | train_loss=0.8584, val_loss=0.7245 | train_acc=0.608, val_acc=0.800 | t_train=0.002s, t_val=0.001s\n",
            "\n",
            "==========================================================================================\n",
            "Entrenando con: Mini-Batch (bs=16)\n",
            "==========================================================================================\n",
            "[ MINI | updates/epoch=8] Epoch 001 | train_loss=1.3547, val_loss=1.1070 | train_acc=0.183, val_acc=0.133 | t_train=0.012s, t_val=0.001s\n",
            "[ MINI | updates/epoch=8] Epoch 010 | train_loss=0.9616, val_loss=0.7912 | train_acc=0.517, val_acc=0.733 | t_train=0.012s, t_val=0.001s\n",
            "[ MINI | updates/epoch=8] Epoch 020 | train_loss=0.8011, val_loss=0.6765 | train_acc=0.667, val_acc=0.800 | t_train=0.013s, t_val=0.001s\n",
            "[ MINI | updates/epoch=8] Epoch 030 | train_loss=0.7365, val_loss=0.6229 | train_acc=0.675, val_acc=0.800 | t_train=0.012s, t_val=0.001s\n",
            "[ MINI | updates/epoch=8] Epoch 040 | train_loss=0.6731, val_loss=0.5820 | train_acc=0.742, val_acc=0.800 | t_train=0.012s, t_val=0.001s\n",
            "[ MINI | updates/epoch=8] Epoch 050 | train_loss=0.6302, val_loss=0.5472 | train_acc=0.775, val_acc=0.800 | t_train=0.012s, t_val=0.001s\n",
            "[ MINI | updates/epoch=8] Epoch 060 | train_loss=0.6058, val_loss=0.5108 | train_acc=0.758, val_acc=0.800 | t_train=0.012s, t_val=0.001s\n",
            "[ MINI | updates/epoch=8] Epoch 070 | train_loss=0.5665, val_loss=0.4827 | train_acc=0.742, val_acc=0.800 | t_train=0.012s, t_val=0.001s\n",
            "[ MINI | updates/epoch=8] Epoch 080 | train_loss=0.5225, val_loss=0.4566 | train_acc=0.742, val_acc=0.800 | t_train=0.012s, t_val=0.001s\n",
            "[ MINI | updates/epoch=8] Epoch 090 | train_loss=0.5151, val_loss=0.4371 | train_acc=0.758, val_acc=0.800 | t_train=0.012s, t_val=0.001s\n",
            "[ MINI | updates/epoch=8] Epoch 100 | train_loss=0.4679, val_loss=0.4214 | train_acc=0.792, val_acc=0.800 | t_train=0.013s, t_val=0.001s\n",
            "[ MINI | updates/epoch=8] Epoch 110 | train_loss=0.4942, val_loss=0.4069 | train_acc=0.758, val_acc=0.800 | t_train=0.012s, t_val=0.001s\n",
            "[ MINI | updates/epoch=8] Epoch 120 | train_loss=0.4252, val_loss=0.3960 | train_acc=0.850, val_acc=0.767 | t_train=0.013s, t_val=0.001s\n",
            "\n",
            "==========================================================================================\n",
            "Entrenando con: SGD (bs=1)\n",
            "==========================================================================================\n",
            "[  SGD | updates/epoch=120] Epoch 001 | train_loss=1.0160, val_loss=0.7004 | train_acc=0.492, val_acc=0.800 | t_train=0.209s, t_val=0.001s\n",
            "[  SGD | updates/epoch=120] Epoch 010 | train_loss=0.4927, val_loss=0.3734 | train_acc=0.725, val_acc=0.800 | t_train=0.176s, t_val=0.003s\n",
            "[  SGD | updates/epoch=120] Epoch 020 | train_loss=0.3478, val_loss=0.2775 | train_acc=0.833, val_acc=0.867 | t_train=0.171s, t_val=0.001s\n",
            "[  SGD | updates/epoch=120] Epoch 030 | train_loss=0.3085, val_loss=0.2072 | train_acc=0.842, val_acc=0.933 | t_train=0.170s, t_val=0.001s\n",
            "[  SGD | updates/epoch=120] Epoch 040 | train_loss=0.1884, val_loss=0.1536 | train_acc=0.900, val_acc=0.933 | t_train=0.212s, t_val=0.001s\n",
            "[  SGD | updates/epoch=120] Epoch 050 | train_loss=0.1419, val_loss=0.1215 | train_acc=0.967, val_acc=0.967 | t_train=0.183s, t_val=0.001s\n",
            "[  SGD | updates/epoch=120] Epoch 060 | train_loss=0.1634, val_loss=0.1126 | train_acc=0.958, val_acc=0.933 | t_train=0.171s, t_val=0.001s\n",
            "[  SGD | updates/epoch=120] Epoch 070 | train_loss=0.1832, val_loss=0.1177 | train_acc=0.950, val_acc=0.967 | t_train=0.172s, t_val=0.001s\n",
            "[  SGD | updates/epoch=120] Epoch 080 | train_loss=0.1691, val_loss=0.1041 | train_acc=0.950, val_acc=0.967 | t_train=0.171s, t_val=0.001s\n",
            "[  SGD | updates/epoch=120] Epoch 090 | train_loss=0.1722, val_loss=0.1107 | train_acc=0.925, val_acc=0.967 | t_train=0.169s, t_val=0.001s\n",
            "[  SGD | updates/epoch=120] Epoch 100 | train_loss=0.1010, val_loss=0.0833 | train_acc=0.975, val_acc=0.967 | t_train=0.175s, t_val=0.001s\n",
            "[  SGD | updates/epoch=120] Epoch 110 | train_loss=0.1632, val_loss=0.1023 | train_acc=0.950, val_acc=0.967 | t_train=0.195s, t_val=0.001s\n",
            "[  SGD | updates/epoch=120] Epoch 120 | train_loss=0.0982, val_loss=0.0848 | train_acc=0.967, val_acc=1.000 | t_train=0.179s, t_val=0.001s\n",
            "\n",
            "--- Resumen Optimización ---\n",
            "Batch GD                 | upd/ep=1   | train_loss=0.8584, val_loss=0.7245 | train_acc=0.608, val_acc=0.800 | T_train=0.27s, T_val=0.10s\n",
            "Mini-Batch (bs=16)       | upd/ep=8   | train_loss=0.4252, val_loss=0.3960 | train_acc=0.850, val_acc=0.767 | T_train=1.53s, T_val=0.10s\n",
            "SGD (bs=1)               | upd/ep=120 | train_loss=0.0982, val_loss=0.0848 | train_acc=0.967, val_acc=1.000 | T_train=21.66s, T_val=0.11s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TDU3rkFQwRjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Pj1DqoHwRPE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}